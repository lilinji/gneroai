---
title: ğŸ’  OpenMPI é›†ç¾¤ä½¿ç”¨æ–¹æ³•
sidebar_position: 10
description: >-
  Parallel computing with OpenMPI on HPC clusters.
---

# ğŸ’  OpenMPI é›†ç¾¤ä½¿ç”¨æ–¹æ³•

OpenMPI æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½ã€å¯æ‰©å±•ã€å¼€æºçš„æ¶ˆæ¯ä¼ é€’æ¥å£ï¼ˆMPIï¼‰å®ç°ï¼Œå¹¿æ³›åº”ç”¨äºé«˜æ€§èƒ½è®¡ç®—ï¼ˆHPCï¼‰é›†ç¾¤ç¯å¢ƒã€‚å®ƒæ”¯æŒ MPI-1ã€MPI-2 å’Œ MPI-3 æ ‡å‡†ï¼Œé€‚ç”¨äºå¤šç§ç¡¬ä»¶å’Œæ“ä½œç³»ç»Ÿå¹³å°ã€‚

æœ¬æ‰‹å†Œå°†æŒ‡å¯¼æ‚¨åœ¨é›†ç¾¤ç¯å¢ƒä¸‹é«˜æ•ˆåœ°ç¼–è¯‘ã€è¿è¡Œå’Œç®¡ç†åŸºäº OpenMPI çš„å¹¶è¡Œç¨‹åºã€‚

---

## ç›®å½•
- [OpenMPI ç®€ä»‹](#openmpi-ç®€ä»‹)
- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [ç¤ºä¾‹ç¨‹åºç¼–å†™](#ç¤ºä¾‹ç¨‹åºç¼–å†™)
- [ä½œä¸šè„šæœ¬ä¸æäº¤](#ä½œä¸šè„šæœ¬ä¸æäº¤)
- [è¿è¡Œä¸è¾“å‡º](#è¿è¡Œä¸è¾“å‡º)
- [åŠ¨ç”»æ¼”ç¤º](#åŠ¨ç”»æ¼”ç¤º)
- [å¸¸è§é—®é¢˜ä¸æ’æŸ¥](#å¸¸è§é—®é¢˜ä¸æ’æŸ¥)
- [å‚è€ƒèµ„æ–™](#å‚è€ƒèµ„æ–™)

---

## OpenMPI ç®€ä»‹

OpenMPIï¼ˆOpen Message Passing Interfaceï¼‰æ˜¯ç”±å¤šä¸ªå¼€æºé¡¹ç›®è”åˆå¼€å‘çš„ MPI å®ç°ï¼Œå…·æœ‰å¦‚ä¸‹ç‰¹ç‚¹ï¼š
- æ”¯æŒå¤šç§ç½‘ç»œäº’è¿ï¼ˆå¦‚ InfiniBandã€Ethernet ç­‰ï¼‰
- é«˜å¯æ‰©å±•æ€§å’Œé«˜æ€§èƒ½
- ä¸°å¯Œçš„è°ƒè¯•ä¸æ€§èƒ½åˆ†æå·¥å…·
- ç¤¾åŒºæ´»è·ƒã€æ–‡æ¡£å®Œå–„

æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ [OpenMPI å®˜ç½‘](https://www.open-mpi.org/)ã€‚

---

## ç¯å¢ƒå‡†å¤‡

1. **åŠ è½½ OpenMPI æ¨¡å—**

é›†ç¾¤é€šå¸¸é€šè¿‡æ¨¡å—ç³»ç»Ÿï¼ˆå¦‚ Lmodã€Environment Modulesï¼‰ç®¡ç†è½¯ä»¶ç¯å¢ƒã€‚è¯·å…ˆåŠ è½½æ‰€éœ€çš„ç¼–è¯‘å™¨å’Œ OpenMPI ç‰ˆæœ¬ï¼š

```bash
module load gcc
module load mpi/openmpi-4.1.6
```

2. **æ£€æŸ¥ MPI ç‰ˆæœ¬**

```bash
mpirun --version
```

---

## ç¤ºä¾‹ç¨‹åºç¼–å†™

ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•ç»“åˆ OpenMPI ä¸ OpenMP å®ç°æ··åˆå¹¶è¡Œï¼Œè®¡ç®—åœ†å‘¨ç‡ Ï€ çš„è¿‘ä¼¼å€¼ã€‚

1. **åˆ›å»ºå·¥ä½œç›®å½•å¹¶è¿›å…¥**

```bash
mkdir openmpitest
cd openmpitest
```

2. **ç¼–å†™æµ‹è¯•ç¨‹åº `openmpitest.c`**

```c
#include <stdio.h>
#include <mpi.h>
#include <omp.h>
#include <math.h>

#define NUM_THREADS 8
long int n = 10000000;

int main(int argc, char *argv[])
{
    int my_rank, numprocs;
    long int i, my_n, my_first_i, my_last_i;
    double my_pi = 0.0, pi, h, x;

    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &numprocs);
    MPI_Comm_rank(MPI_COMM_WORLD, &my_rank);
    h = 1.0 / n;
    my_n = n / numprocs;
    my_first_i = my_rank * my_n;
    my_last_i = my_first_i + my_n;

    omp_set_num_threads(NUM_THREADS);
#pragma omp parallel for reduction(+ : my_pi) private(x, i)
    for (i = my_first_i; i < my_last_i; i++)
    {
        x = (i + 0.5) * h;
        my_pi = my_pi + 4.0 / (1.0 + x * x);
    }

    MPI_Reduce(&my_pi, &pi, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD);

    if (my_rank == 0)
    {
        printf("Approximation of pi:%15.13f\n", pi * h);
    }

    MPI_Finalize();
    return 0;
}
```

---

## ä½œä¸šè„šæœ¬ä¸æäº¤

åœ¨é›†ç¾¤ä¸Šå»ºè®®é€šè¿‡ä½œä¸šè°ƒåº¦ç³»ç»Ÿï¼ˆå¦‚ Slurmï¼‰æäº¤ MPI ä½œä¸šã€‚

1. **ç¼–å†™ Slurm ä½œä¸šè„šæœ¬ `openmpitest.slurm`**

```bash
#!/bin/bash
#SBATCH --job-name=openmpitest
#SBATCH --partition=64c512g
#SBATCH --ntasks-per-node=1
#SBATCH -n 4
#SBATCH --output=%j.out
#SBATCH --error=%j.err

ulimit -s unlimited
ulimit -l unlimited

module load gcc
module load mpi/openmpi-4.1.6

mpicc openmpitest.c -o openmpitest -fopenmp

mpirun -np 4 ./openmpitest
```

2. **æäº¤ä½œä¸š**

```bash
sbatch openmpitest.slurm
```

---

## è¿è¡Œä¸è¾“å‡º

ä½œä¸šå®Œæˆåï¼Œå¯åœ¨è¾“å‡ºæ–‡ä»¶ï¼ˆå¦‚ `123456.out`ï¼‰ä¸­æŸ¥çœ‹ç»“æœï¼š

```text
Approximation of pi:3.1415926535898
```

---

## åŠ¨ç”»æ¼”ç¤º

ä¸‹æ–¹ä¸º OpenMPI ä½œä¸šæäº¤ä¸è¿è¡Œçš„å®é™…æ“ä½œæ¼”ç¤ºï¼š

![openmpi.gif](/static/guides/hpc/software-applications/openmpi.gif)


---

## å¸¸è§é—®é¢˜ä¸æ’æŸ¥

### 1. ä½œä¸šæœªæ­£å¸¸è¿è¡Œ/æ— è¾“å‡º
- æ£€æŸ¥ä½œä¸šè„šæœ¬ä¸­çš„æ¨¡å—åŠ è½½ã€è·¯å¾„ã€å‚æ•°è®¾ç½®æ˜¯å¦æ­£ç¡®
- æŸ¥çœ‹ `.err` æ–‡ä»¶è·å–è¯¦ç»†æŠ¥é”™ä¿¡æ¯

### 2. mpirun æŠ¥é”™â€œcommand not foundâ€
- ç¡®è®¤å·²æ­£ç¡®åŠ è½½ OpenMPI æ¨¡å—
- ä½¿ç”¨ `which mpirun` æ£€æŸ¥è·¯å¾„

### 3. æ€§èƒ½ä¸ç†æƒ³
- åˆç†è®¾ç½® `--ntasks`ã€`--cpus-per-task` ç­‰å‚æ•°
- æ£€æŸ¥èŠ‚ç‚¹é—´ç½‘ç»œçŠ¶å†µ
- ä½¿ç”¨ OpenMPI è‡ªå¸¦çš„ `--report-bindings`ã€`--map-by` ç­‰å‚æ•°ä¼˜åŒ–è¿›ç¨‹ç»‘å®š

### 4. ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
- ç¡®è®¤ç¼–è¯‘å’Œè¿è¡Œæ—¶åŠ è½½çš„ OpenMPI ç‰ˆæœ¬ä¸€è‡´

---

## å‚è€ƒèµ„æ–™
- [OpenMPI å®˜æ–¹æ–‡æ¡£](https://www.open-mpi.org/doc/)
- [Slurm å®˜æ–¹æ–‡æ¡£](https://slurm.schedmd.com/documentation.html)
- [HPC Best Practices](https://docs.nersc.gov/development/parallel-computing/mpi/)

---

<head>
  <title>OpenMPI é›†ç¾¤ä½¿ç”¨æ–¹æ³• Guide</title>
  <meta
    name="description"
    content="Parallel computing with OpenMPI on HPC clusters."
  />
</head> 